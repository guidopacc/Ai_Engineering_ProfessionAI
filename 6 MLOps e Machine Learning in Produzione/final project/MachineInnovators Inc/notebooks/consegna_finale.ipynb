{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w9T0cgSZo8T"
   },
   "source": [
    "# Sentiment Reputation Monitoring - Report di Consegna\n",
    "\n",
    "Questo documento presenta il progetto completo per il monitoraggio della reputazione online tramite analisi del sentiment su testi provenienti da social media e feedback clienti.\n",
    "\n",
    "Il sistema implementa una pipeline end-to-end che va dalla raccolta di feedback testuali fino alla visualizzazione di metriche operative su dashboard, passando per l'inferenza di sentiment analysis, il deployment containerizzato e il monitoring continuo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IySZbBUKZo8U"
   },
   "source": [
    "## 1. Introduzione\n",
    "\n",
    "### Contesto\n",
    "\n",
    "Il progetto nasce dall'esigenza di un'azienda di monitorare in tempo reale la propria reputazione online. Ogni giorno arrivano centinaia di feedback: post su LinkedIn, recensioni su forum B2B, commenti su gruppi specializzati. Un cliente segnala problematiche sull'affidabilità dell'azienda, un altro elogia i tempi di consegna. Come trasformare questo flusso continuo di informazioni non strutturate in insight operativi?\n",
    "\n",
    "### Obiettivi\n",
    "\n",
    "L'obiettivo principale è costruire un sistema che:\n",
    "\n",
    "- Analizzi automaticamente il sentiment di testi provenienti da diverse fonti\n",
    "- Esponga un'API REST per integrazione con altri sistemi\n",
    "- Tracci metriche operative per monitoring continuo\n",
    "- Rilevi automaticamente quando la distribuzione dei feedback cambia (concept drift)\n",
    "- Sia deployabile facilmente con Docker e orchestrato con CI/CD\n",
    "\n",
    "### Repository GitHub\n",
    "\n",
    "Il codice completo del progetto è disponibile su GitHub:\n",
    "\n",
    "**https://github.com/guidopacc/Ai_Engineering_ProfessionAI/tree/main/6%20MLOps%20e%20Machine%20Learning%20in%20Produzione**\n",
    "\n",
    "Il repository include tutto il codice sorgente, i test, la configurazione Docker, i workflow CI/CD e la documentazione completa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sdk_S9zkZo8U"
   },
   "source": [
    "## 2. Setup Ambiente\n",
    "\n",
    "Per eseguire questo notebook su Google Colab, installiamo le dipendenze essenziali. Nota: in produzione si usa un ambiente virtuale Python e un Dockerfile completo, ma per questa dimostrazione ci limitiamo ai pacchetti necessari per l'inferenza e i test API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dsIKXtbgZo8V"
   },
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn transformers torch requests prometheus-client pytest -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fe9Hnr24Zo8V",
    "outputId": "0fce0286-1530-40a0-ba13-6418a106132a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creato file samples.jsonl con 5 campioni\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un file di esempio con alcuni campioni per la dimostrazione\n",
    "import json\n",
    "\n",
    "samples = [\n",
    "    {\"text\": \"I love this product! It's amazing and works perfectly.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"This is terrible. I'm very disappointed.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"The product arrived on time. It's okay.\", \"label\": \"neutral\"},\n",
    "    {\"text\": \"Best purchase ever! Highly recommend to everyone.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Not satisfied with the quality. Poor service.\", \"label\": \"negative\"}\n",
    "]\n",
    "\n",
    "with open(\"samples.jsonl\", \"w\") as f:\n",
    "    for sample in samples:\n",
    "        f.write(json.dumps(sample) + \"\\n\")\n",
    "\n",
    "print(f\"Creato file samples.jsonl con {len(samples)} campioni\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGBvJuusZo8V"
   },
   "source": [
    "## 3. Dimostrazione Inferenza\n",
    "\n",
    "Il modello utilizzato è `cardiffnlp/twitter-roberta-base-sentiment-latest`, un RoBERTa base fine-tunato su tweet per sentiment analysis. Questo modello è particolarmente adatto per analizzare linguaggio informale e slang tipico dei social media.\n",
    "\n",
    "Carichiamo il modello e testiamo alcune predizioni su esempi realistici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DVryythhZo8V",
    "outputId": "6fd810c8-df21-4daa-ad9c-1d6157c05dd7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467,
     "referenced_widgets": [
      "323674a9717f44088be3b9d674033c9c",
      "f31d8981e4ba487ab3d1581009e63662",
      "0c23dadc56a74e0a8df135c9ba751225",
      "5751c5d1f23c4f5c9b4758b37029e42e",
      "699abf86b73f4e769af82ea5b9b6c17b",
      "3715f35d9c86415998d56e4be2d1a48a",
      "13601216e8e0462a94aefc7308a33472",
      "81f0942da0734422bd7f531dc4dfeed0",
      "54fcf38292064650870ba7149395fd21",
      "0290c5a709054980995598511ae74978",
      "adc382197702482fbb2e1578b5138d7f",
      "f7984dc3745b45a3b05b07293b35bf07",
      "912292b99bc14ee08b3c1b22ac4e05bd",
      "98757f0a652b413fa3c089f7bc936443",
      "c8d3d61539d74941b9e9ed8b31526cd7",
      "2d15e2b307d6493fb1c55cb012532651",
      "e721db251a97494a983691bef251947c",
      "2f7eabc3f12b441f918febf24bd06e67",
      "938458ed8b7d48569800d8f774aa87e0",
      "c5f19e42bd5b4e4988735f07ab2d9898",
      "5f5e8eac2cb2474fa8febd8ef1d92bc5",
      "850213a94ed147f29b2f7703764258ce",
      "1e242e3bb5cd48afb1e62d45418fd98e",
      "90ed3a16fdaf4ec0ae9d4fc6a52c7bdd",
      "3b5ceb0daaa14730918512548c09de46",
      "6e937d4dd7034b378db410efa98dd033",
      "8bf4c090d4164840b636ef4ab3761585",
      "a7c89d542ece4085973b3c389dd0bea1",
      "fcffd700aa7547319fbbc73a1d293e01",
      "ad104de9b63c4054a578e986e5309ad1",
      "f6df624a086c499ba20d988f2058a7bb",
      "d567dd0776884d1aa75427fdd7f426ac",
      "f10c1a7a9112434da252324a6be5e021",
      "baad313f1d504bf48e2c373d25c3965d",
      "5807fd8dc2b14364a02f0848540e3f97",
      "3951a56e5dfc44b59c164ae721eb6959",
      "1717df620c3b418d9c2159509ad08577",
      "9478031389fc436c97f12b3f9a5d471f",
      "b6e3c3015b1f4b62b99564e4da5b5091",
      "2cc01bbdc9de47ee9d71a876bece64e1",
      "d4dd94e1a6934d028709bf7d5b461574",
      "77f59d592a4c4fc8a92e010c5db6dd74",
      "754cde7c049a443789e51a5fb963c118",
      "5d2d6e37bf224c6786de10a973dc2fe2",
      "d7d98516940b4d6c80696debc1c9c1ab",
      "cf23741b37f04b60816ff3a977060520",
      "7f82897b44064fe28f367e9fe389d532",
      "81ab8f733c0b4a7ab8978daf21ecbce3",
      "93e348e0ea014730ba761723b0b5c87e",
      "d8227c08fee142baa1ab8536aa6e38f9",
      "d43cb48b5bb24069ba46d7b84bcb95ef",
      "bed87bb2a53d4421bb1ad9aa283ae4fa",
      "d86f21af6660414bb8a3ce5f3808cf20",
      "9230df4ccf4449eab6529fd7a2ffde0f",
      "efb247b5de1245e693c00bb14b659a62",
      "8263dd1aac5f40fca1d64fa5215247ce",
      "a246bc0b71614585b8047a3e5cab3bdf",
      "6340f8e9753b4a6aa1c73258a962e8f0",
      "244a8c866f3e49189025bb93332b1dcc",
      "2e22451aafd84e268ce1e19238438b91",
      "6a903d1b4eba401a9d745c0287edaed6",
      "102cf4d642c440e3a8dbf388c9042614",
      "14b6a2cd9ab84b078d24a3da7d63e6f0",
      "5b76e636eb144058864a9d24f39a442b",
      "448cfb17124c4b8e97cab365e2a0d7fe",
      "8891a1ec576f4a19a8fed52984ac7c31"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Caricamento modello...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323674a9717f44088be3b9d674033c9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7984dc3745b45a3b05b07293b35bf07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e242e3bb5cd48afb1e62d45418fd98e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "baad313f1d504bf48e2c373d25c3965d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7d98516940b4d6c80696debc1c9c1ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8263dd1aac5f40fca1d64fa5215247ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Modello caricato con successo!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Caricamento del modello (la prima volta può richiedere qualche minuto per il download)\n",
    "print(\"Caricamento modello...\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=False,\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")\n",
    "print(\"Modello caricato con successo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4RnX9UmGZo8V",
    "outputId": "0c964147-18ba-4fc0-bb3b-875ee545a431",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Risultati predizioni:\n",
      "\n",
      "Testo: Ottimi ingranaggi, consegna puntuale. Consigliato!\n",
      "Sentiment: positive (confidenza: 71.54%)\n",
      "\n",
      "Testo: Prodotto arrivato rotto. Assistenza clienti non risponde.\n",
      "Sentiment: neutral (confidenza: 75.43%)\n",
      "\n",
      "Testo: Ingranaggi funzionano, nulla di eccezionale ma ok per il prezzo.\n",
      "Sentiment: neutral (confidenza: 69.79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funzione per normalizzare le label del modello\n",
    "def normalize_label(label):\n",
    "    \"\"\"Normalizza le label del modello in formato standardizzato.\"\"\"\n",
    "    label_lower = label.lower()\n",
    "    if \"negative\" in label_lower or label_lower == \"label_0\":\n",
    "        return \"negative\"\n",
    "    elif \"neutral\" in label_lower or label_lower == \"label_1\":\n",
    "        return \"neutral\"\n",
    "    elif \"positive\" in label_lower or label_lower == \"label_2\":\n",
    "        return \"positive\"\n",
    "    return label_lower\n",
    "\n",
    "# Test su alcuni esempi\n",
    "test_texts = [\n",
    "    \"Ottimi ingranaggi, consegna puntuale. Consigliato!\",\n",
    "    \"Prodotto arrivato rotto. Assistenza clienti non risponde.\",\n",
    "    \"Ingranaggi funzionano, nulla di eccezionale ma ok per il prezzo.\"\n",
    "]\n",
    "\n",
    "print(\"Risultati predizioni:\\n\")\n",
    "for text in test_texts:\n",
    "    result = sentiment_pipeline(text)\n",
    "    label_raw = result[0][\"label\"]\n",
    "    score = result[0][\"score\"]\n",
    "    label = normalize_label(label_raw)\n",
    "\n",
    "    print(f\"Testo: {text}\")\n",
    "    print(f\"Sentiment: {label} (confidenza: {score:.2%})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETTpX7D9Zo8W"
   },
   "source": [
    "### Come interpretare i risultati\n",
    "\n",
    "Il modello restituisce una label (negative, neutral, positive) e uno score di confidenza tra 0 e 1. Uno score alto (es. > 0.9) indica che il modello è molto sicuro della sua predizione. Uno score basso (es. < 0.6) suggerisce che il testo potrebbe essere ambiguo o contenere sentiment misti.\n",
    "\n",
    "Il modello può sbagliare quando:\n",
    "- Il testo contiene sarcasmo o ironia\n",
    "- Il contesto è ambiguo o mancano informazioni\n",
    "- Il linguaggio è molto tecnico o settoriale\n",
    "- Ci sono errori di ortografia significativi\n",
    "\n",
    "Per questo motivo, in produzione è importante monitorare la distribuzione dei sentiment nel tempo e rilevare quando cambia significativamente rispetto alla baseline storica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_FtE8lbZo8W"
   },
   "source": [
    "## 4. Architettura del Progetto\n",
    "\n",
    "Il sistema è organizzato in diversi componenti che lavorano insieme per fornire una soluzione completa di monitoring della reputazione.\n",
    "\n",
    "### Flusso End-to-End\n",
    "\n",
    "```\n",
    "Feedback Social (LinkedIn, Forum, etc.)\n",
    "    ↓\n",
    "FastAPI API (/predict, /predict/batch)\n",
    "    ↓\n",
    "Modello Hugging Face (sentiment analysis)\n",
    "    ↓\n",
    "Metriche Prometheus (/metrics)\n",
    "    ↓\n",
    "Prometheus (scraping ogni 5 secondi)\n",
    "    ↓\n",
    "Grafana Dashboard (visualizzazione)\n",
    "```\n",
    "\n",
    "### Componenti Principali\n",
    "\n",
    "**FastAPI**: Framework web moderno per servire il modello come API REST. Offre validazione automatica con Pydantic, documentazione auto-generata e performance asincrone.\n",
    "\n",
    "**Pytest**: Suite di test automatici che verifica il corretto funzionamento degli endpoint e della logica di inferenza. I test vengono eseguiti automaticamente ad ogni commit tramite GitHub Actions.\n",
    "\n",
    "**Docker + Docker Compose**: Containerizzazione dell'applicazione per deployment consistente. Docker Compose orchestra tre servizi: API, Prometheus e Grafana.\n",
    "\n",
    "**CI/CD con GitHub Actions**: Pipeline automatizzata che esegue test ad ogni push e build/push dell'immagine Docker su GitHub Container Registry ad ogni release.\n",
    "\n",
    "**Prometheus + Grafana**: Sistema di monitoring che raccoglie metriche operative (request rate, latenza, distribuzione sentiment) e le visualizza su dashboard interattive.\n",
    "\n",
    "**Drift Detection**: Implementazione di KL divergence per rilevare quando la distribuzione dei sentiment cambia rispetto a una baseline, segnalando possibili problemi o cambiamenti nel comportamento dei clienti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8phm2ZKQZo8W"
   },
   "source": [
    "## 5. Estratti di Codice Significativi\n",
    "\n",
    "Di seguito alcuni estratti di codice che mostrano le scelte tecniche principali del progetto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhvx8g1CZo8W"
   },
   "source": [
    "### Endpoint FastAPI con Validazione Pydantic\n",
    "\n",
    "L'endpoint `/predict` utilizza Pydantic per validare automaticamente l'input. Se il testo è vuoto o troppo lungo, FastAPI restituisce un errore 422 senza eseguire l'inferenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HqO7X7BWZo8W",
    "outputId": "475ba998-1005-4d67-a863-d5ffb07dcec7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Schema Pydantic definito: validazione automatica di input/output\n"
     ]
    }
   ],
   "source": [
    "# Esempio di schema Pydantic e endpoint FastAPI\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import FastAPI\n",
    "\n",
    "class TextItem(BaseModel):\n",
    "    \"\"\"Schema per richiesta di predizione singola.\"\"\"\n",
    "    text: str = Field(..., min_length=1, max_length=1000)\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    \"\"\"Schema per risposta.\"\"\"\n",
    "    label: str\n",
    "    score: float = Field(..., ge=0.0, le=1.0)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\", response_model=Prediction)\n",
    "async def predict(item: TextItem):\n",
    "    \"\"\"Predice il sentiment per un singolo testo.\"\"\"\n",
    "    # La validazione è automatica: testo vuoto o > 1000 caratteri → 422\n",
    "    # In produzione qui chiameremmo predict_one(item.text)\n",
    "    return Prediction(label=\"positive\", score=0.95)\n",
    "\n",
    "print(\"Schema Pydantic definito: validazione automatica di input/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cxw1n8ylZo8X"
   },
   "source": [
    "### Dockerfile\n",
    "\n",
    "Il Dockerfile crea un'immagine leggera partendo da `python:3.10-slim`. Le dipendenze vengono installate prima del codice sorgente per sfruttare la cache di Docker e velocizzare rebuild successivi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zSPWqZWHZo8X",
    "outputId": "aaeeab1b-559d-448d-84b5-8e03c63e97bd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dockerfile:\n",
      "\n",
      "# Dockerfile per sentiment reputation monitoring API\n",
      "FROM python:3.10-slim\n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "COPY src/ ./src/\n",
      "\n",
      "EXPOSE 8000\n",
      "\n",
      "CMD [\"uvicorn\", \"src.app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dockerfile_content = \"\"\"\n",
    "# Dockerfile per sentiment reputation monitoring API\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY src/ ./src/\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"uvicorn\", \"src.app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Dockerfile:\")\n",
    "print(dockerfile_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcQ9JF-FZo8X"
   },
   "source": [
    "### Metriche Prometheus\n",
    "\n",
    "Le metriche vengono esposte in formato Prometheus compatibile. Un Counter traccia il numero totale di richieste, un Histogram misura la latenza, e un Counter separato traccia la distribuzione dei sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rFwgb3ftZo8X",
    "outputId": "43f21c16-e29a-49bf-a504-96d00758d587",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Metriche Prometheus definite:\n",
      "- fastapi_requests_total: conta richieste per metodo/endpoint/status\n",
      "- fastapi_request_latency_seconds: misura latenza con bucket predefiniti\n",
      "- sentiment_predictions_total: conta predizioni per label\n"
     ]
    }
   ],
   "source": [
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Definizione metriche Prometheus\n",
    "REQUEST_COUNT = Counter(\n",
    "    'fastapi_requests_total',\n",
    "    'Total number of requests',\n",
    "    ['method', 'endpoint', 'status']\n",
    ")\n",
    "\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'fastapi_request_latency_seconds',\n",
    "    'Request latency in seconds',\n",
    "    ['method', 'endpoint'],\n",
    "    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    ")\n",
    "\n",
    "SENTIMENT_DISTRIBUTION = Counter(\n",
    "    'sentiment_predictions_total',\n",
    "    'Total sentiment predictions by label',\n",
    "    ['label']\n",
    ")\n",
    "\n",
    "print(\"Metriche Prometheus definite:\")\n",
    "print(\"- fastapi_requests_total: conta richieste per metodo/endpoint/status\")\n",
    "print(\"- fastapi_request_latency_seconds: misura latenza con bucket predefiniti\")\n",
    "print(\"- sentiment_predictions_total: conta predizioni per label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpfy7eOPZo8X"
   },
   "source": [
    "### Test Pytest\n",
    "\n",
    "I test verificano che gli endpoint rispondano correttamente e che la validazione funzioni. Usiamo `TestClient` di FastAPI per simulare richieste HTTP senza avviare un server reale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lK-YmIWEZo8X",
    "outputId": "72e6219d-c412-495a-cfc9-5df3506f2591",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Esempio test Pytest:\n",
      "\n",
      "from fastapi.testclient import TestClient\n",
      "from src.app.main import app\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "def test_predict_endpoint():\n",
      "    \"\"\"Test endpoint /predict.\"\"\"\n",
      "    response = client.post(\n",
      "        \"/predict\",\n",
      "        json={\"text\": \"I love this product!\"}\n",
      "    )\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    data = response.json()\n",
      "    assert \"label\" in data\n",
      "    assert \"score\" in data\n",
      "    assert data[\"label\"] in [\"negative\", \"neutral\", \"positive\"]\n",
      "    assert 0.0 <= data[\"score\"] <= 1.0\n",
      "\n",
      "def test_predict_endpoint_invalid():\n",
      "    \"\"\"Test validazione input.\"\"\"\n",
      "    response = client.post(\"/predict\", json={\"text\": \"\"})\n",
      "    assert response.status_code == 422  # Validation error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_code = '''\n",
    "from fastapi.testclient import TestClient\n",
    "from src.app.main import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_predict_endpoint():\n",
    "    \"\"\"Test endpoint /predict.\"\"\"\n",
    "    response = client.post(\n",
    "        \"/predict\",\n",
    "        json={\"text\": \"I love this product!\"}\n",
    "    )\n",
    "\n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"label\" in data\n",
    "    assert \"score\" in data\n",
    "    assert data[\"label\"] in [\"negative\", \"neutral\", \"positive\"]\n",
    "    assert 0.0 <= data[\"score\"] <= 1.0\n",
    "\n",
    "def test_predict_endpoint_invalid():\n",
    "    \"\"\"Test validazione input.\"\"\"\n",
    "    response = client.post(\"/predict\", json={\"text\": \"\"})\n",
    "    assert response.status_code == 422  # Validation error\n",
    "'''\n",
    "\n",
    "print(\"Esempio test Pytest:\")\n",
    "print(test_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LV14tDDZo8X"
   },
   "source": [
    "### Pipeline CI/CD con GitHub Actions\n",
    "\n",
    "Il workflow CI si attiva su ogni pull request e push su main. Esegue i test automaticamente e builda l'immagine Docker per validazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eikDA95XZo8X",
    "outputId": "e5ff6b0a-8e0c-4290-8586-802116e4f813",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Workflow CI GitHub Actions:\n",
      "\n",
      "name: CI\n",
      "\n",
      "on:\n",
      "  pull_request:\n",
      "    branches: [main, master]\n",
      "  push:\n",
      "    branches: [main, master]\n",
      "\n",
      "jobs:\n",
      "  test:\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "      - uses: actions/checkout@v4\n",
      "      - uses: actions/setup-python@v5\n",
      "        with:\n",
      "          python-version: \"3.10\"\n",
      "      - run: pip install -r requirements.txt\n",
      "      - run: pytest --cov=src\n",
      "      - run: docker build -f docker/Dockerfile -t test-image .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ci_workflow = '''\n",
    "name: CI\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    branches: [main, master]\n",
    "  push:\n",
    "    branches: [main, master]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.10\"\n",
    "      - run: pip install -r requirements.txt\n",
    "      - run: pytest --cov=src\n",
    "      - run: docker build -f docker/Dockerfile -t test-image .\n",
    "'''\n",
    "\n",
    "print(\"Workflow CI GitHub Actions:\")\n",
    "print(ci_workflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg1Ae6t3Zo8X"
   },
   "source": [
    "## 6. Risultati e Test\n",
    "\n",
    "I test del progetto coprono tre aree principali: test unitari per le funzioni di inferenza, test di integrazione per gli endpoint API, e test di health check. Tutti i test passano correttamente.\n",
    "\n",
    "### Output Test\n",
    "\n",
    "Eseguendo `pytest` sul progetto completo si ottiene:\n",
    "\n",
    "```\n",
    "============================= test session starts ==============================\n",
    "collected 12 items\n",
    "\n",
    "tests/test_api_integration.py .....                                      [ 41%]\n",
    "tests/test_health.py .                                                   [ 50%]\n",
    "tests/test_infer_unit.py ......                                          [100%]\n",
    "\n",
    "======================== 12 passed, 3 warnings in 6.46s ========================\n",
    "```\n",
    "\n",
    "### Esempio Output Endpoint\n",
    "\n",
    "**GET /health:**\n",
    "```json\n",
    "{\n",
    "  \"status\": \"ok\",\n",
    "  \"model_loaded\": true,\n",
    "  \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "}\n",
    "```\n",
    "\n",
    "**POST /predict:**\n",
    "```json\n",
    "{\n",
    "  \"label\": \"positive\",\n",
    "  \"score\": 0.9382082223892212\n",
    "}\n",
    "```\n",
    "\n",
    "**GET /metrics:**\n",
    "Output in formato Prometheus exposition format con metriche come:\n",
    "- `fastapi_requests_total{method=\"POST\",endpoint=\"/predict\",status=\"200\"} 42`\n",
    "- `fastapi_request_latency_seconds_bucket{method=\"POST\",endpoint=\"/predict\",le=\"0.5\"} 38`\n",
    "- `sentiment_predictions_total{label=\"positive\"} 25`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3hm-8aKcZo8X",
    "outputId": "9f4a1a5e-ff84-486e-d629-cd62047c3fd4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Simulazione Chiamate API ===\n",
      "\n",
      "1. GET /health\n",
      "   Response: {\"status\": \"ok\", \"model_loaded\": true, \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}\n",
      "\n",
      "2. POST /predict\n",
      "   Request: {\"text\": \"Great quality gears!\"}\n",
      "   Response: {\"label\": \"positive\", \"score\": 0.94}\n",
      "\n",
      "3. GET /metrics\n",
      "   Response: Formato Prometheus con metriche esposte\n",
      "   - fastapi_requests_total\n",
      "   - fastapi_request_latency_seconds\n",
      "   - sentiment_predictions_total\n"
     ]
    }
   ],
   "source": [
    "# Simulazione di chiamate API per dimostrazione\n",
    "print(\"=== Simulazione Chiamate API ===\\n\")\n",
    "\n",
    "# Health check\n",
    "print(\"1. GET /health\")\n",
    "print('   Response: {\"status\": \"ok\", \"model_loaded\": true, \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}')\n",
    "print()\n",
    "\n",
    "# Predict\n",
    "print(\"2. POST /predict\")\n",
    "print('   Request: {\"text\": \"Great quality gears!\"}')\n",
    "print('   Response: {\"label\": \"positive\", \"score\": 0.94}')\n",
    "print()\n",
    "\n",
    "# Metrics\n",
    "print(\"3. GET /metrics\")\n",
    "print(\"   Response: Formato Prometheus con metriche esposte\")\n",
    "print(\"   - fastapi_requests_total\")\n",
    "print(\"   - fastapi_request_latency_seconds\")\n",
    "print(\"   - sentiment_predictions_total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5kdYW0jZo8X"
   },
   "source": [
    "## 7. Sviluppi Futuri e Miglioramenti\n",
    "\n",
    "Il sistema attuale funziona bene per un caso d'uso base, ma ci sono diverse aree di miglioramento che potrebbero essere implementate in futuro.\n",
    "\n",
    "**Fine-tuning del modello**: Il modello pre-addestrato funziona bene su testi generici, ma potrebbe essere migliorato con un fine-tuning su feedback specifici del settore meccanico. Questo richiederebbe la raccolta di un dataset etichettato di feedback reali dell'azienda.\n",
    "\n",
    "**Integrazione con fonti dati reali**: Attualmente il sistema riceve testi via API, ma in produzione servirebbe integrazione diretta con API di social media (LinkedIn, Twitter) o scraping periodico di forum e recensioni. Questo richiederebbe gestione di autenticazione, rate limiting e parsing di formati diversi.\n",
    "\n",
    "**Rate limiting e sicurezza**: Per proteggere l'API da abusi, servirebbe implementare rate limiting per IP e autenticazione con API key. FastAPI supporta middleware per questo scopo.\n",
    "\n",
    "**Caching delle predizioni**: Testi identici o molto simili vengono riprocessati ogni volta. Un sistema di caching (es. Redis) potrebbe ridurre la latenza e il carico sul modello per richieste frequenti.\n",
    "\n",
    "**Dashboard Grafana più dettagliata**: La dashboard attuale mostra metriche base. Potrebbe essere estesa con alerting automatico quando il drift supera una soglia, visualizzazioni di trend storici, e correlazione con eventi esterni (es. lanci di prodotto, campagne marketing).\n",
    "\n",
    "**Versioning del modello**: Quando si fa retraining o si cambia modello, servirebbe un sistema per gestire versioni multiple e fare A/B testing prima di sostituire completamente il modello in produzione.\n",
    "\n",
    "**Retraining automatico**: Un DAG Airflow potrebbe schedulare retraining periodico (es. mensile) usando nuovi dati raccolti, validare le performance su un test set, e deployare automaticamente se le metriche migliorano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StR6AC4-Zo8Y"
   },
   "source": [
    "## 8. Conclusioni\n",
    "\n",
    "Questo progetto dimostra come costruire una pipeline MLOps completa per un caso d'uso reale di sentiment analysis. Abbiamo visto come integrare un modello Hugging Face pre-addestrato in un'API FastAPI, come esporre metriche per monitoring, come containerizzare l'applicazione con Docker, e come automatizzare test e deployment con CI/CD.\n",
    "\n",
    "Il sistema è utile perché trasforma dati non strutturati (feedback testuali) in metriche operative monitorabili, permettendo all'azienda di reagire rapidamente a cambiamenti nella percezione del brand o a problemi emergenti. Il monitoring continuo e il drift detection aiutano a mantenere il sistema affidabile nel tempo.\n",
    "\n",
    "L'approccio modulare e containerizzato rende il sistema facilmente deployabile e scalabile. La pipeline CI/CD garantisce che ogni modifica sia testata automaticamente prima di essere rilasciata in produzione.\n",
    "\n",
    "Il codice completo, inclusi test, configurazione Docker, workflow CI/CD e documentazione, è disponibile sul repository GitHub:\n",
    "\n",
    "**https://github.com/guidopacc/Ai_Engineering_ProfessionAI/tree/main/6%20MLOps%20e%20Machine%20Learning%20in%20Produzione**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}