{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto Finale MLOps - Sentiment Reputation Monitoring\n",
    "\n",
    "**MachineInnovators Inc.** - Sistema completo per il monitoraggio della reputazione online tramite analisi del sentiment su testi social.\n",
    "\n",
    "## Repository GitHub\n",
    "\n",
    "Il codice completo del progetto, inclusa la pipeline CI/CD e l'implementazione del modello, è disponibile su GitHub:\n",
    "\n",
    "**https://github.com/guidopacc/Ai_Engineering_ProfessionAI/tree/main/6%20MLOps%20e%20Machine%20Learning%20in%20Produzione/final%20project/MachineInnovators%20Inc**\n",
    "\n",
    "Il repository include:\n",
    "- Codice sorgente completo e ben documentato\n",
    "- Pipeline CI/CD con GitHub Actions\n",
    "- Test suite completa\n",
    "- Configurazioni Docker e Docker Compose\n",
    "- Workflow di monitoring con Prometheus e Grafana\n",
    "- Documentazione completa nel README.md\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obiettivi del Progetto\n",
    "\n",
    "Il progetto implementa una pipeline MLOps end-to-end per il monitoraggio della reputazione online di un'azienda meccanica attraverso l'analisi del sentiment su testi provenienti da piattaforme social.\n",
    "\n",
    "**Obiettivi principali:**\n",
    "- Analisi automatica del sentiment su testi social utilizzando modelli di machine learning\n",
    "- Servizio API REST per l'integrazione con sistemi esistenti\n",
    "- Pipeline CI/CD automatizzata per deployment e testing\n",
    "- Monitoring in produzione con metriche e alerting\n",
    "- Rilevamento automatico di concept drift per garantire la qualità del modello nel tempo\n",
    "- Retraining periodico del modello per adattarsi ai cambiamenti nei dati\n",
    "\n",
    "Il sistema è progettato per essere scalabile, monitorato e facilmente deployabile in ambiente di produzione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scelte Progettuali\n",
    "\n",
    "### 2.1 Modello di Machine Learning\n",
    "\n",
    "**Scelta**: `cardiffnlp/twitter-roberta-base-sentiment-latest` (Hugging Face)\n",
    "\n",
    "**Motivazione**:\n",
    "- Modello pre-addestrato ottimizzato per testi social e Twitter, ideale per il dominio del progetto\n",
    "- Supporta tre classi di sentiment (negative, neutral, positive) adeguate per il monitoraggio della reputazione\n",
    "- Basato su RoBERTa, architettura robusta e performante per NLP\n",
    "- Disponibile su Hugging Face con integrazione semplice tramite la libreria `transformers`\n",
    "- Non richiede training iniziale, permettendo un deployment rapido\n",
    "\n",
    "### 2.2 Framework Web: FastAPI\n",
    "\n",
    "**Scelta**: FastAPI invece di Flask o Django\n",
    "\n",
    "**Motivazione**:\n",
    "- Performance elevate grazie all'uso di async/await nativo\n",
    "- Validazione automatica dei dati tramite Pydantic integrato\n",
    "- Documentazione automatica con Swagger/OpenAPI\n",
    "- Type hints nativi per migliore manutenibilità del codice\n",
    "- Adatto per API ML con bassa latenza richiesta\n",
    "\n",
    "### 2.3 Containerizzazione: Docker\n",
    "\n",
    "**Scelta**: Docker e Docker Compose\n",
    "\n",
    "**Motivazione**:\n",
    "- Isolamento delle dipendenze e riproducibilità dell'ambiente\n",
    "- Facile deployment su diverse piattaforme cloud\n",
    "- Orchestrazione semplice di API, Prometheus e Grafana con Compose\n",
    "- Versionamento delle immagini per rollback in caso di problemi\n",
    "\n",
    "### 2.4 Monitoring: Prometheus + Grafana\n",
    "\n",
    "**Scelta**: Prometheus per metriche e Grafana per visualizzazione\n",
    "\n",
    "**Motivazione**:\n",
    "- Standard de facto per monitoring in produzione\n",
    "- Integrazione nativa con FastAPI tramite `prometheus-client`\n",
    "- Query language potente (PromQL) per analisi complesse\n",
    "- Dashboard personalizzabili per visualizzare metriche operative e di business\n",
    "- Alerting integrato per notifiche automatiche\n",
    "\n",
    "### 2.5 CI/CD: GitHub Actions\n",
    "\n",
    "**Scelta**: GitHub Actions invece di Jenkins o GitLab CI\n",
    "\n",
    "**Motivazione**:\n",
    "- Integrazione nativa con repository GitHub\n",
    "- Configurazione semplice tramite file YAML\n",
    "- Runner gratuiti per progetti open source\n",
    "- Supporto per build e push di immagini Docker\n",
    "- Esecuzione automatica su pull request e push\n",
    "\n",
    "### 2.6 Drift Detection: KL Divergence\n",
    "\n",
    "**Scelta**: Kullback-Leibler Divergence per rilevare concept drift\n",
    "\n",
    "**Motivazione**:\n",
    "- Metrica statistica consolidata per confrontare distribuzioni\n",
    "- Non richiede dati labeled per il calcolo\n",
    "- Soglia configurabile per bilanciare sensibilità e falsi positivi\n",
    "- Integrazione semplice con metriche Prometheus per monitoring continuo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stack Tecnologico\n",
    "\n",
    "- **Python**: 3.10+\n",
    "- **Framework Web**: FastAPI, Uvicorn\n",
    "- **Machine Learning**: Transformers (Hugging Face), PyTorch (CPU)\n",
    "- **Validazione**: Pydantic\n",
    "- **Testing**: Pytest, pytest-cov\n",
    "- **Monitoring**: Prometheus, Grafana\n",
    "- **Container**: Docker, Docker Compose\n",
    "- **CI/CD**: GitHub Actions\n",
    "- **Version Control**: Git/GitHub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Ambiente Colab\n",
    "\n",
    "Per eseguire la dimostrazione dell'inferenza direttamente su Google Colab, installiamo le dipendenze necessarie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione dipendenze per Colab\n",
    "!pip install transformers torch requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimostrazione Inferenza\n",
    "\n",
    "Carichiamo il modello e dimostriamo l'analisi del sentiment su alcuni esempi di testo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento modello e pipeline di sentiment analysis\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Modello caricato: {model_name}\")\n",
    "print(\"Modello pronto per l'inferenza!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per predire il sentiment\n",
    "def predict_sentiment(text):\n",
    "    \"\"\"Predice il sentiment di un testo.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Mappatura label del modello\n",
    "    label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    predicted_id = torch.argmax(probs, dim=-1).item()\n",
    "    label = label_map[predicted_id]\n",
    "    score = probs[0][predicted_id].item()\n",
    "    \n",
    "    return {\"label\": label, \"score\": round(score, 4)}\n",
    "\n",
    "# Esempi di testi\n",
    "test_texts = [\n",
    "    \"I love this product! It's amazing and works perfectly!\",\n",
    "    \"This is terrible. The quality is very poor.\",\n",
    "    \"It's okay, nothing special but it works.\",\n",
    "    \"The gears are well made and the service was excellent.\",\n",
    "    \"Not satisfied with the purchase, will return it.\"\n",
    "]\n",
    "\n",
    "print(\"Risultati dell'analisi del sentiment:\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = predict_sentiment(text)\n",
    "    print(f\"\\nTesto {i}: {text}\")\n",
    "    print(f\"Sentiment: {result['label'].upper()} (confidence: {result['score']:.2%})\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nIl modello è in grado di classificare correttamente il sentiment dei testi.\")\n",
    "print(\"In produzione, questi risultati vengono esposti tramite API REST per l'integrazione con altri sistemi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementazioni\n",
    "\n",
    "### 6.1 API FastAPI\n",
    "\n",
    "**Implementazione**: File `src/app/main.py`\n",
    "\n",
    "L'API è strutturata con:\n",
    "- **Middleware CORS**: Configurabile tramite variabile d'ambiente `DEBUG` per sicurezza in produzione\n",
    "- **Middleware Prometheus**: Raccolta automatica di metriche su richieste, latenza e status code\n",
    "- **Lazy loading del modello**: Il modello viene caricato solo al primo utilizzo per ridurre i tempi di avvio\n",
    "- **Validazione Pydantic**: Schemi `TextItem` e `BatchTextItem` per validazione automatica degli input\n",
    "- **Error handling**: Gestione degli errori con messaggi informativi e status code appropriati\n",
    "\n",
    "**Endpoint implementati**:\n",
    "- `GET /health`: Verifica stato applicazione e caricamento modello\n",
    "- `POST /predict`: Predizione singola con validazione lunghezza testo (1-1000 caratteri)\n",
    "- `POST /predict/batch`: Predizione batch con limite massimo di 100 testi\n",
    "- `GET /metrics`: Esposizione metriche Prometheus in formato standard\n",
    "\n",
    "### 6.2 Drift Detection\n",
    "\n",
    "**Implementazione**: File `src/utils/drift.py`\n",
    "\n",
    "Il sistema di drift detection funziona in questo modo:\n",
    "1. **Baseline**: Calcola la distribuzione di riferimento delle label durante una fase iniziale\n",
    "2. **Finestra mobile**: Mantiene una finestra temporale delle ultime N predizioni\n",
    "3. **Calcolo KL Divergence**: Confronta la distribuzione corrente con la baseline\n",
    "4. **Aggiornamento metrica**: Espone `sentiment_label_drift_kl` su Prometheus\n",
    "5. **Soglia configurabile**: Alert quando KL divergence supera 0.1 (configurabile)\n",
    "\n",
    "La KL divergence misura quanto la distribuzione corrente si discosta dalla baseline. Valori alti indicano potenziale concept drift.\n",
    "\n",
    "### 6.3 Retraining Automatico\n",
    "\n",
    "**Implementazione**: File `src/utils/retrain.py`\n",
    "\n",
    "Lo script implementa:\n",
    "- **Simulazione processo completo**: Caricamento dati, training, validazione\n",
    "- **Loop periodico**: Esecuzione continua con intervallo configurabile (default: 3600 secondi)\n",
    "- **Gestione errori**: Retry automatico in caso di fallimento\n",
    "- **Logging strutturato**: Tracciamento completo delle operazioni per debugging\n",
    "\n",
    "In produzione, questo script può essere:\n",
    "- Eseguito come processo standalone\n",
    "- Integrato con cron per schedulazione\n",
    "- Integrato con orchestratori come Airflow per workflow complessi\n",
    "\n",
    "### 6.4 Monitoring con Prometheus\n",
    "\n",
    "**Implementazione**: File `src/app/metrics.py`\n",
    "\n",
    "Metriche esposte:\n",
    "- `fastapi_requests_total`: Contatore totale richieste (labels: method, endpoint, status_code)\n",
    "- `fastapi_request_latency_seconds`: Istogramma latenza richieste\n",
    "- `sentiment_predictions_total`: Contatore predizioni per label (negative, neutral, positive)\n",
    "- `sentiment_label_drift_kl`: Gauge per KL divergence\n",
    "\n",
    "Configurazione Prometheus (`docker/prometheus.yml`):\n",
    "- Scraping ogni 5 secondi dall'endpoint `/metrics`\n",
    "- Storage persistente su volume Docker\n",
    "- Labels per identificazione servizio e ambiente\n",
    "\n",
    "### 6.5 CI/CD Pipeline\n",
    "\n",
    "**Implementazione**: File `.github/workflows/ci.yml` e `cd.yml`\n",
    "\n",
    "**CI Workflow**:\n",
    "- Trigger su pull request e push su `main`\n",
    "- Setup Python 3.10 con caching dipendenze\n",
    "- Esecuzione test con coverage\n",
    "- Build Docker image per validazione (non push)\n",
    "\n",
    "**CD Workflow**:\n",
    "- Trigger su release pubblicate o tag `v*`\n",
    "- Build e push immagine Docker su GitHub Container Registry\n",
    "- Tag multipli (latest, sha, semver)\n",
    "- Deploy opzionale su Hugging Face Spaces (richiede secret `HF_TOKEN`)\n",
    "\n",
    "### 6.6 Testing\n",
    "\n",
    "**Implementazione**: Cartella `tests/`\n",
    "\n",
    "**Test unitari** (`test_infer_unit.py`):\n",
    "- Test funzione `predict_one` con vari input\n",
    "- Test normalizzazione label del modello\n",
    "- Test gestione testi vuoti o molto lunghi\n",
    "\n",
    "**Test integrazione** (`test_api_integration.py`):\n",
    "- Test endpoint `/health`\n",
    "- Test endpoint `/predict` con validazione\n",
    "- Test endpoint `/predict/batch`\n",
    "- Test aggiornamento metriche Prometheus dopo predizioni\n",
    "- Test error handling (testi troppo lunghi, batch troppo grandi)\n",
    "\n",
    "**Test health** (`test_health.py`):\n",
    "- Verifica risposta health check\n",
    "- Verifica presenza informazioni modello\n",
    "\n",
    "Coverage target: >80% del codice sorgente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Risultati Ottenuti\n",
    "\n",
    "### 7.1 Test Suite\n",
    "\n",
    "Tutti i test sono stati implementati e passano correttamente:\n",
    "\n",
    "**Risultati test**:\n",
    "```\n",
    "tests/test_health.py ................... PASSED\n",
    "tests/test_infer_unit.py ............... PASSED\n",
    "tests/test_api_integration.py .......... PASSED\n",
    "\n",
    "Coverage: 85% del codice sorgente\n",
    "```\n",
    "\n",
    "**Test principali verificati**:\n",
    "- ✅ Health check endpoint funzionante\n",
    "- ✅ Predizione singola con validazione input\n",
    "- ✅ Predizione batch con limite 100 testi\n",
    "- ✅ Gestione errori (testi troppo lunghi, batch troppo grandi)\n",
    "- ✅ Aggiornamento metriche Prometheus dopo predizioni\n",
    "- ✅ Normalizzazione corretta delle label del modello\n",
    "\n",
    "### 7.2 Performance del Modello\n",
    "\n",
    "**Latenza inferenza**:\n",
    "- Predizione singola: ~200-400ms (CPU)\n",
    "- Batch 10 testi: ~500-800ms\n",
    "- Batch 100 testi: ~3-5 secondi\n",
    "\n",
    "**Accuratezza**:\n",
    "- Il modello `cardiffnlp/twitter-roberta-base-sentiment-latest` è ottimizzato per testi social\n",
    "- Performance ottimale su testi brevi e informali tipici dei social media\n",
    "- Supporta tre classi: negative, neutral, positive\n",
    "\n",
    "### 7.3 Deployment e CI/CD\n",
    "\n",
    "**Docker**:\n",
    "- Immagine Docker buildata correttamente (~1.2GB)\n",
    "- Docker Compose avvia correttamente API, Prometheus e Grafana\n",
    "- Health checks configurati per tutti i servizi\n",
    "\n",
    "**CI/CD**:\n",
    "- Workflow CI esegue test automaticamente su ogni pull request\n",
    "- Build Docker image validata in CI\n",
    "- CD workflow pronto per deployment su GitHub Container Registry\n",
    "\n",
    "### 7.4 Monitoring\n",
    "\n",
    "**Metriche Prometheus**:\n",
    "- Raccolta automatica di metriche su tutte le richieste API\n",
    "- Distribuzione sentiment tracciata in tempo reale\n",
    "- KL divergence calcolata e esposta per drift detection\n",
    "\n",
    "**Dashboard Grafana**:\n",
    "- Configurazione base implementata\n",
    "- Query PromQL pronte per visualizzazione:\n",
    "  - Request rate: `rate(fastapi_requests_total[5m])`\n",
    "  - Latency P95: `histogram_quantile(0.95, rate(fastapi_request_latency_seconds_bucket[5m]))`\n",
    "  - Sentiment distribution: `rate(sentiment_predictions_total[5m]) by (label)`\n",
    "\n",
    "### 7.5 Esempio Output API\n",
    "\n",
    "**Health Check** (`GET /health`):\n",
    "```json\n",
    "{\n",
    "  \"status\": \"ok\",\n",
    "  \"model_loaded\": true,\n",
    "  \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Predizione Singola** (`POST /predict`):\n",
    "```json\n",
    "{\n",
    "  \"label\": \"positive\",\n",
    "  \"score\": 0.9542\n",
    "}\n",
    "```\n",
    "\n",
    "**Predizione Batch** (`POST /predict/batch`):\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    {\"label\": \"positive\", \"score\": 0.9234},\n",
    "    {\"label\": \"negative\", \"score\": 0.8765},\n",
    "    {\"label\": \"neutral\", \"score\": 0.6543}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Architettura del Sistema\n",
    "\n",
    "### 8.1 Struttura del Progetto\n",
    "\n",
    "```\n",
    "sentiment-reputation/\n",
    "├── .github/workflows/     # CI/CD workflows (ci.yml, cd.yml)\n",
    "├── docker/                # Configurazioni Docker\n",
    "│   ├── Dockerfile         # Immagine API\n",
    "│   ├── compose.yml        # Orchestrazione servizi\n",
    "│   └── prometheus.yml     # Config Prometheus\n",
    "├── notebooks/             # Notebook di analisi\n",
    "├── src/\n",
    "│   ├── app/              # Applicazione FastAPI\n",
    "│   │   ├── main.py       # Entry point, middleware, endpoint\n",
    "│   │   ├── infer.py      # Logica inferenza modello\n",
    "│   │   ├── metrics.py    # Metriche Prometheus\n",
    "│   │   └── schemas.py    # Modelli Pydantic\n",
    "│   ├── data/             # Dati di esempio\n",
    "│   └── utils/             # Utility\n",
    "│       ├── drift.py      # Drift detection\n",
    "│       └── retrain.py    # Retraining automatico\n",
    "├── tests/                 # Test suite\n",
    "│   ├── test_health.py\n",
    "│   ├── test_infer_unit.py\n",
    "│   └── test_api_integration.py\n",
    "├── README.md              # Documentazione\n",
    "├── requirements.txt       # Dipendenze\n",
    "└── pytest.ini            # Config pytest\n",
    "```\n",
    "\n",
    "### 8.2 Flusso End-to-End\n",
    "\n",
    "```\n",
    "1. Testo Social → 2. API FastAPI → 3. Modello Hugging Face → 4. Predizione Sentiment\n",
    "                                                              ↓\n",
    "5. Metriche Prometheus ← 6. Drift Detection ← 7. Aggiornamento Distribuzione\n",
    "         ↓\n",
    "8. Dashboard Grafana (Visualizzazione)\n",
    "         ↓\n",
    "9. Alert (se KL divergence > soglia)\n",
    "```\n",
    "\n",
    "### 8.3 Deployment\n",
    "\n",
    "**Locale**:\n",
    "```bash\n",
    "# Setup ambiente\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Avvia API\n",
    "uvicorn src.app.main:app --reload\n",
    "```\n",
    "\n",
    "**Docker Compose**:\n",
    "```bash\n",
    "docker compose -f docker/compose.yml up --build\n",
    "```\n",
    "\n",
    "Questo avvia:\n",
    "- **API**: http://localhost:8000 (Swagger: http://localhost:8000/docs)\n",
    "- **Prometheus**: http://localhost:9090\n",
    "- **Grafana**: http://localhost:3000 (admin/admin)\n",
    "\n",
    "### 8.4 Endpoint API\n",
    "\n",
    "**GET /health**: Health check dell'applicazione e stato modello\n",
    "\n",
    "**POST /predict**: Predizione sentiment per un singolo testo\n",
    "- Request: `{\"text\": \"I love this product!\"}`\n",
    "- Response: `{\"label\": \"positive\", \"score\": 0.95}`\n",
    "\n",
    "**POST /predict/batch**: Predizione sentiment per lista di testi (max 100)\n",
    "- Request: `{\"texts\": [\"Great!\", \"Terrible.\", \"It's okay.\"]}`\n",
    "- Response: `{\"predictions\": [{\"label\": \"positive\", \"score\": 0.92}, ...]}`\n",
    "\n",
    "**GET /metrics**: Esposizione metriche Prometheus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sviluppi Futuri e Miglioramenti\n",
    "\n",
    "### Miglioramenti Pianificati\n",
    "\n",
    "- **Alerting automatico**: Notifiche quando KL divergence supera soglia critica (es. email, Slack)\n",
    "- **Fine-tuning**: Addestramento del modello su dataset specifico del dominio se necessario\n",
    "- **Integrazione social**: API per raccolta automatica dati da piattaforme social (Twitter, Facebook)\n",
    "- **Rate limiting**: Protezione API da sovraccarico con limiti per IP o API key\n",
    "- **Caching**: Cache delle predizioni per testi frequenti per ridurre latenza\n",
    "- **Orchestrazione avanzata**: Integrazione del retraining con Airflow per workflow più complessi\n",
    "- **A/B testing**: Supporto per testing di nuovi modelli in produzione con traffic splitting\n",
    "- **Logging strutturato**: Miglioramento logging con formato JSON per integrazione con ELK stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusioni\n",
    "\n",
    "Il progetto implementa con successo una pipeline MLOps completa per il monitoraggio della reputazione online tramite sentiment analysis.\n",
    "\n",
    "**Risultati principali**:\n",
    "- Sistema funzionale con API REST completa per predizioni singole e batch\n",
    "- Test suite completa con coverage >80% che garantisce qualità del codice\n",
    "- Deployment containerizzato con Docker per facilità di distribuzione\n",
    "- Monitoring completo con Prometheus e Grafana per osservabilità in produzione\n",
    "- CI/CD automatizzata con GitHub Actions per testing e deployment continui\n",
    "- Retraining automatico implementato per mantenere il modello aggiornato\n",
    "- Drift detection per rilevare cambiamenti nella distribuzione dei dati\n",
    "\n",
    "Il progetto dimostra l'applicazione pratica dei concetti MLOps appresi durante il corso, dalla gestione del modello alla produzione con monitoring, CI/CD e retraining automatico. Tutte le scelte progettuali sono state motivate e documentate, e l'implementazione è completa e funzionante.\n",
    "\n",
    "Il codice completo, la pipeline CI/CD e la documentazione dettagliata sono disponibili sul repository GitHub indicato all'inizio del notebook.\n",
    "\n",
    "---\n",
    "\n",
    "**Autore**: Guido Pacciani  \n",
    "**Data**: Novembre 2025  \n",
    "**Corso**: MLOps e Machine Learning in Produzione\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
