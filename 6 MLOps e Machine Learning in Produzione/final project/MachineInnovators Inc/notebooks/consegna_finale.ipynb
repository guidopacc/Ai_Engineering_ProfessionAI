{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Reputation Monitoring - Report di Consegna\n",
        "\n",
        "Questo documento presenta il progetto completo per il monitoraggio della reputazione online tramite analisi del sentiment su testi provenienti da social media e feedback clienti.\n",
        "\n",
        "Il sistema implementa una pipeline end-to-end che va dalla raccolta di feedback testuali fino alla visualizzazione di metriche operative su dashboard, passando per l'inferenza di sentiment analysis, il deployment containerizzato e il monitoring continuo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduzione\n",
        "\n",
        "### Contesto\n",
        "\n",
        "Il progetto nasce dall'esigenza di un'azienda meccanica di monitorare in tempo reale la propria reputazione online. Ogni giorno arrivano centinaia di feedback: post su LinkedIn, recensioni su forum B2B, commenti su gruppi specializzati. Un cliente segnala rumorosità eccessiva su un modello di ingranaggio, un altro elogia i tempi di consegna. Come trasformare questo flusso continuo di informazioni non strutturate in insight operativi?\n",
        "\n",
        "### Obiettivi\n",
        "\n",
        "L'obiettivo principale è costruire un sistema che:\n",
        "\n",
        "- Analizzi automaticamente il sentiment di testi provenienti da diverse fonti\n",
        "- Esponga un'API REST per integrazione con altri sistemi\n",
        "- Tracci metriche operative per monitoring continuo\n",
        "- Rilevi automaticamente quando la distribuzione dei feedback cambia (concept drift)\n",
        "- Sia deployabile facilmente con Docker e orchestrato con CI/CD\n",
        "\n",
        "### Repository GitHub\n",
        "\n",
        "Il codice completo del progetto è disponibile su GitHub:\n",
        "\n",
        "**https://github.com/yourusername/sentiment-reputation**\n",
        "\n",
        "Il repository include tutto il codice sorgente, i test, la configurazione Docker, i workflow CI/CD e la documentazione completa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup Ambiente\n",
        "\n",
        "Per eseguire questo notebook su Google Colab, installiamo le dipendenze essenziali. Nota: in produzione si usa un ambiente virtuale Python e un Dockerfile completo, ma per questa dimostrazione ci limitiamo ai pacchetti necessari per l'inferenza e i test API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn transformers torch requests prometheus-client pytest -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creiamo un file di esempio con alcuni campioni per la dimostrazione\n",
        "import json\n",
        "\n",
        "samples = [\n",
        "    {\"text\": \"I love this product! It's amazing and works perfectly.\", \"label\": \"positive\"},\n",
        "    {\"text\": \"This is terrible. I'm very disappointed.\", \"label\": \"negative\"},\n",
        "    {\"text\": \"The product arrived on time. It's okay.\", \"label\": \"neutral\"},\n",
        "    {\"text\": \"Best purchase ever! Highly recommend to everyone.\", \"label\": \"positive\"},\n",
        "    {\"text\": \"Not satisfied with the quality. Poor service.\", \"label\": \"negative\"}\n",
        "]\n",
        "\n",
        "with open(\"samples.jsonl\", \"w\") as f:\n",
        "    for sample in samples:\n",
        "        f.write(json.dumps(sample) + \"\\n\")\n",
        "\n",
        "print(f\"Creato file samples.jsonl con {len(samples)} campioni\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dimostrazione Inferenza\n",
        "\n",
        "Il modello utilizzato è `cardiffnlp/twitter-roberta-base-sentiment-latest`, un RoBERTa base fine-tunato su tweet per sentiment analysis. Questo modello è particolarmente adatto per analizzare linguaggio informale e slang tipico dei social media.\n",
        "\n",
        "Carichiamo il modello e testiamo alcune predizioni su esempi realistici.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Caricamento del modello (la prima volta può richiedere qualche minuto per il download)\n",
        "print(\"Caricamento modello...\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    return_all_scores=False,\n",
        "    max_length=512,\n",
        "    truncation=True\n",
        ")\n",
        "print(\"Modello caricato con successo!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funzione per normalizzare le label del modello\n",
        "def normalize_label(label):\n",
        "    \"\"\"Normalizza le label del modello in formato standardizzato.\"\"\"\n",
        "    label_lower = label.lower()\n",
        "    if \"negative\" in label_lower or label_lower == \"label_0\":\n",
        "        return \"negative\"\n",
        "    elif \"neutral\" in label_lower or label_lower == \"label_1\":\n",
        "        return \"neutral\"\n",
        "    elif \"positive\" in label_lower or label_lower == \"label_2\":\n",
        "        return \"positive\"\n",
        "    return label_lower\n",
        "\n",
        "# Test su alcuni esempi\n",
        "test_texts = [\n",
        "    \"Ottimi ingranaggi, consegna puntuale. Consigliato!\",\n",
        "    \"Prodotto arrivato rotto. Assistenza clienti non risponde.\",\n",
        "    \"Ingranaggi funzionano, nulla di eccezionale ma ok per il prezzo.\"\n",
        "]\n",
        "\n",
        "print(\"Risultati predizioni:\\n\")\n",
        "for text in test_texts:\n",
        "    result = sentiment_pipeline(text)\n",
        "    label_raw = result[0][\"label\"]\n",
        "    score = result[0][\"score\"]\n",
        "    label = normalize_label(label_raw)\n",
        "    \n",
        "    print(f\"Testo: {text}\")\n",
        "    print(f\"Sentiment: {label} (confidenza: {score:.2%})\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Come interpretare i risultati\n",
        "\n",
        "Il modello restituisce una label (negative, neutral, positive) e uno score di confidenza tra 0 e 1. Uno score alto (es. > 0.9) indica che il modello è molto sicuro della sua predizione. Uno score basso (es. < 0.6) suggerisce che il testo potrebbe essere ambiguo o contenere sentiment misti.\n",
        "\n",
        "Il modello può sbagliare quando:\n",
        "- Il testo contiene sarcasmo o ironia\n",
        "- Il contesto è ambiguo o mancano informazioni\n",
        "- Il linguaggio è molto tecnico o settoriale\n",
        "- Ci sono errori di ortografia significativi\n",
        "\n",
        "Per questo motivo, in produzione è importante monitorare la distribuzione dei sentiment nel tempo e rilevare quando cambia significativamente rispetto alla baseline storica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Architettura del Progetto\n",
        "\n",
        "Il sistema è organizzato in diversi componenti che lavorano insieme per fornire una soluzione completa di monitoring della reputazione.\n",
        "\n",
        "### Flusso End-to-End\n",
        "\n",
        "```\n",
        "Feedback Social (LinkedIn, Forum, etc.)\n",
        "    ↓\n",
        "FastAPI API (/predict, /predict/batch)\n",
        "    ↓\n",
        "Modello Hugging Face (sentiment analysis)\n",
        "    ↓\n",
        "Metriche Prometheus (/metrics)\n",
        "    ↓\n",
        "Prometheus (scraping ogni 5 secondi)\n",
        "    ↓\n",
        "Grafana Dashboard (visualizzazione)\n",
        "```\n",
        "\n",
        "### Componenti Principali\n",
        "\n",
        "**FastAPI**: Framework web moderno per servire il modello come API REST. Offre validazione automatica con Pydantic, documentazione auto-generata e performance asincrone.\n",
        "\n",
        "**Pytest**: Suite di test automatici che verifica il corretto funzionamento degli endpoint e della logica di inferenza. I test vengono eseguiti automaticamente ad ogni commit tramite GitHub Actions.\n",
        "\n",
        "**Docker + Docker Compose**: Containerizzazione dell'applicazione per deployment consistente. Docker Compose orchestra tre servizi: API, Prometheus e Grafana.\n",
        "\n",
        "**CI/CD con GitHub Actions**: Pipeline automatizzata che esegue test ad ogni push e build/push dell'immagine Docker su GitHub Container Registry ad ogni release.\n",
        "\n",
        "**Prometheus + Grafana**: Sistema di monitoring che raccoglie metriche operative (request rate, latenza, distribuzione sentiment) e le visualizza su dashboard interattive.\n",
        "\n",
        "**Drift Detection**: Implementazione di KL divergence per rilevare quando la distribuzione dei sentiment cambia rispetto a una baseline, segnalando possibili problemi o cambiamenti nel comportamento dei clienti.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estratti di Codice Significativi\n",
        "\n",
        "Di seguito alcuni estratti di codice che mostrano le scelte tecniche principali del progetto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Endpoint FastAPI con Validazione Pydantic\n",
        "\n",
        "L'endpoint `/predict` utilizza Pydantic per validare automaticamente l'input. Se il testo è vuoto o troppo lungo, FastAPI restituisce un errore 422 senza eseguire l'inferenza.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esempio di schema Pydantic e endpoint FastAPI\n",
        "from pydantic import BaseModel, Field\n",
        "from fastapi import FastAPI\n",
        "\n",
        "class TextItem(BaseModel):\n",
        "    \"\"\"Schema per richiesta di predizione singola.\"\"\"\n",
        "    text: str = Field(..., min_length=1, max_length=1000)\n",
        "\n",
        "class Prediction(BaseModel):\n",
        "    \"\"\"Schema per risposta.\"\"\"\n",
        "    label: str\n",
        "    score: float = Field(..., ge=0.0, le=1.0)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/predict\", response_model=Prediction)\n",
        "async def predict(item: TextItem):\n",
        "    \"\"\"Predice il sentiment per un singolo testo.\"\"\"\n",
        "    # La validazione è automatica: testo vuoto o > 1000 caratteri → 422\n",
        "    # In produzione qui chiameremmo predict_one(item.text)\n",
        "    return Prediction(label=\"positive\", score=0.95)\n",
        "\n",
        "print(\"Schema Pydantic definito: validazione automatica di input/output\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dockerfile\n",
        "\n",
        "Il Dockerfile crea un'immagine leggera partendo da `python:3.10-slim`. Le dipendenze vengono installate prima del codice sorgente per sfruttare la cache di Docker e velocizzare rebuild successivi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dockerfile_content = \"\"\"\n",
        "# Dockerfile per sentiment reputation monitoring API\n",
        "FROM python:3.10-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "COPY src/ ./src/\n",
        "\n",
        "EXPOSE 8000\n",
        "\n",
        "CMD [\"uvicorn\", \"src.app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "\"\"\"\n",
        "\n",
        "print(\"Dockerfile:\")\n",
        "print(dockerfile_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metriche Prometheus\n",
        "\n",
        "Le metriche vengono esposte in formato Prometheus compatibile. Un Counter traccia il numero totale di richieste, un Histogram misura la latenza, e un Counter separato traccia la distribuzione dei sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prometheus_client import Counter, Histogram\n",
        "\n",
        "# Definizione metriche Prometheus\n",
        "REQUEST_COUNT = Counter(\n",
        "    'fastapi_requests_total',\n",
        "    'Total number of requests',\n",
        "    ['method', 'endpoint', 'status']\n",
        ")\n",
        "\n",
        "REQUEST_LATENCY = Histogram(\n",
        "    'fastapi_request_latency_seconds',\n",
        "    'Request latency in seconds',\n",
        "    ['method', 'endpoint'],\n",
        "    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
        ")\n",
        "\n",
        "SENTIMENT_DISTRIBUTION = Counter(\n",
        "    'sentiment_predictions_total',\n",
        "    'Total sentiment predictions by label',\n",
        "    ['label']\n",
        ")\n",
        "\n",
        "print(\"Metriche Prometheus definite:\")\n",
        "print(\"- fastapi_requests_total: conta richieste per metodo/endpoint/status\")\n",
        "print(\"- fastapi_request_latency_seconds: misura latenza con bucket predefiniti\")\n",
        "print(\"- sentiment_predictions_total: conta predizioni per label\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Pytest\n",
        "\n",
        "I test verificano che gli endpoint rispondano correttamente e che la validazione funzioni. Usiamo `TestClient` di FastAPI per simulare richieste HTTP senza avviare un server reale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_code = '''\n",
        "from fastapi.testclient import TestClient\n",
        "from src.app.main import app\n",
        "\n",
        "client = TestClient(app)\n",
        "\n",
        "def test_predict_endpoint():\n",
        "    \"\"\"Test endpoint /predict.\"\"\"\n",
        "    response = client.post(\n",
        "        \"/predict\",\n",
        "        json={\"text\": \"I love this product!\"}\n",
        "    )\n",
        "    \n",
        "    assert response.status_code == 200\n",
        "    data = response.json()\n",
        "    assert \"label\" in data\n",
        "    assert \"score\" in data\n",
        "    assert data[\"label\"] in [\"negative\", \"neutral\", \"positive\"]\n",
        "    assert 0.0 <= data[\"score\"] <= 1.0\n",
        "\n",
        "def test_predict_endpoint_invalid():\n",
        "    \"\"\"Test validazione input.\"\"\"\n",
        "    response = client.post(\"/predict\", json={\"text\": \"\"})\n",
        "    assert response.status_code == 422  # Validation error\n",
        "'''\n",
        "\n",
        "print(\"Esempio test Pytest:\")\n",
        "print(test_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline CI/CD con GitHub Actions\n",
        "\n",
        "Il workflow CI si attiva su ogni pull request e push su main. Esegue i test automaticamente e builda l'immagine Docker per validazione.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ci_workflow = '''\n",
        "name: CI\n",
        "\n",
        "on:\n",
        "  pull_request:\n",
        "    branches: [main, master]\n",
        "  push:\n",
        "    branches: [main, master]\n",
        "\n",
        "jobs:\n",
        "  test:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v4\n",
        "      - uses: actions/setup-python@v5\n",
        "        with:\n",
        "          python-version: \"3.10\"\n",
        "      - run: pip install -r requirements.txt\n",
        "      - run: pytest --cov=src\n",
        "      - run: docker build -f docker/Dockerfile -t test-image .\n",
        "'''\n",
        "\n",
        "print(\"Workflow CI GitHub Actions:\")\n",
        "print(ci_workflow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Risultati e Test\n",
        "\n",
        "I test del progetto coprono tre aree principali: test unitari per le funzioni di inferenza, test di integrazione per gli endpoint API, e test di health check. Tutti i test passano correttamente.\n",
        "\n",
        "### Output Test\n",
        "\n",
        "Eseguendo `pytest` sul progetto completo si ottiene:\n",
        "\n",
        "```\n",
        "============================= test session starts ==============================\n",
        "collected 12 items\n",
        "\n",
        "tests/test_api_integration.py .....                                      [ 41%]\n",
        "tests/test_health.py .                                                   [ 50%]\n",
        "tests/test_infer_unit.py ......                                          [100%]\n",
        "\n",
        "======================== 12 passed, 3 warnings in 6.46s ========================\n",
        "```\n",
        "\n",
        "### Esempio Output Endpoint\n",
        "\n",
        "**GET /health:**\n",
        "```json\n",
        "{\n",
        "  \"status\": \"ok\",\n",
        "  \"model_loaded\": true,\n",
        "  \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "}\n",
        "```\n",
        "\n",
        "**POST /predict:**\n",
        "```json\n",
        "{\n",
        "  \"label\": \"positive\",\n",
        "  \"score\": 0.9382082223892212\n",
        "}\n",
        "```\n",
        "\n",
        "**GET /metrics:**\n",
        "Output in formato Prometheus exposition format con metriche come:\n",
        "- `fastapi_requests_total{method=\"POST\",endpoint=\"/predict\",status=\"200\"} 42`\n",
        "- `fastapi_request_latency_seconds_bucket{method=\"POST\",endpoint=\"/predict\",le=\"0.5\"} 38`\n",
        "- `sentiment_predictions_total{label=\"positive\"} 25`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulazione di chiamate API per dimostrazione\n",
        "print(\"=== Simulazione Chiamate API ===\\n\")\n",
        "\n",
        "# Health check\n",
        "print(\"1. GET /health\")\n",
        "print('   Response: {\"status\": \"ok\", \"model_loaded\": true, \"model_name\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"}')\n",
        "print()\n",
        "\n",
        "# Predict\n",
        "print(\"2. POST /predict\")\n",
        "print('   Request: {\"text\": \"Great quality gears!\"}')\n",
        "print('   Response: {\"label\": \"positive\", \"score\": 0.94}')\n",
        "print()\n",
        "\n",
        "# Metrics\n",
        "print(\"3. GET /metrics\")\n",
        "print(\"   Response: Formato Prometheus con metriche esposte\")\n",
        "print(\"   - fastapi_requests_total\")\n",
        "print(\"   - fastapi_request_latency_seconds\")\n",
        "print(\"   - sentiment_predictions_total\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sviluppi Futuri e Miglioramenti\n",
        "\n",
        "Il sistema attuale funziona bene per un caso d'uso base, ma ci sono diverse aree di miglioramento che potrebbero essere implementate in futuro.\n",
        "\n",
        "**Fine-tuning del modello**: Il modello pre-addestrato funziona bene su testi generici, ma potrebbe essere migliorato con un fine-tuning su feedback specifici del settore meccanico. Questo richiederebbe la raccolta di un dataset etichettato di feedback reali dell'azienda.\n",
        "\n",
        "**Integrazione con fonti dati reali**: Attualmente il sistema riceve testi via API, ma in produzione servirebbe integrazione diretta con API di social media (LinkedIn, Twitter) o scraping periodico di forum e recensioni. Questo richiederebbe gestione di autenticazione, rate limiting e parsing di formati diversi.\n",
        "\n",
        "**Rate limiting e sicurezza**: Per proteggere l'API da abusi, servirebbe implementare rate limiting per IP e autenticazione con API key. FastAPI supporta middleware per questo scopo.\n",
        "\n",
        "**Caching delle predizioni**: Testi identici o molto simili vengono riprocessati ogni volta. Un sistema di caching (es. Redis) potrebbe ridurre la latenza e il carico sul modello per richieste frequenti.\n",
        "\n",
        "**Dashboard Grafana più dettagliata**: La dashboard attuale mostra metriche base. Potrebbe essere estesa con alerting automatico quando il drift supera una soglia, visualizzazioni di trend storici, e correlazione con eventi esterni (es. lanci di prodotto, campagne marketing).\n",
        "\n",
        "**Versioning del modello**: Quando si fa retraining o si cambia modello, servirebbe un sistema per gestire versioni multiple e fare A/B testing prima di sostituire completamente il modello in produzione.\n",
        "\n",
        "**Retraining automatico**: Un DAG Airflow potrebbe schedulare retraining periodico (es. mensile) usando nuovi dati raccolti, validare le performance su un test set, e deployare automaticamente se le metriche migliorano.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusioni\n",
        "\n",
        "Questo progetto dimostra come costruire una pipeline MLOps completa per un caso d'uso reale di sentiment analysis. Abbiamo visto come integrare un modello Hugging Face pre-addestrato in un'API FastAPI, come esporre metriche per monitoring, come containerizzare l'applicazione con Docker, e come automatizzare test e deployment con CI/CD.\n",
        "\n",
        "Il sistema è utile perché trasforma dati non strutturati (feedback testuali) in metriche operative monitorabili, permettendo all'azienda di reagire rapidamente a cambiamenti nella percezione del brand o a problemi emergenti. Il monitoring continuo e il drift detection aiutano a mantenere il sistema affidabile nel tempo.\n",
        "\n",
        "L'approccio modulare e containerizzato rende il sistema facilmente deployabile e scalabile. La pipeline CI/CD garantisce che ogni modifica sia testata automaticamente prima di essere rilasciata in produzione.\n",
        "\n",
        "Il codice completo, inclusi test, configurazione Docker, workflow CI/CD e documentazione, è disponibile sul repository GitHub al link indicato nella sezione Introduzione.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
